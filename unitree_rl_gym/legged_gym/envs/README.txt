base:包含基础环境类和机器人类的实现
--legged_robot.py 定义四足机器人的基类，包括机器人的初始化、状态更新、奖励计算等 其中_init_buffers定义许多变量，二次开发于此加
--legged_robot_config.py 机器人配置类、用于定义机器人的物理参数、控制参数等
--base_task 定义许多环境变量，比如观测、奖励、回合长度的buffer


--------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------
每个机器人环境由一个xx.py和一个xx_config.py定义

地形设置在xx_config.py中以mesh_type指定：
mesh_type = 'trimesh' # "heightfield" # none, plane, heightfield or trimesh
none：不使用任何地形网格，地形是完全平坦的。
plane：使用一个平坦的平面作为地形。
heightfield：使用二维高度场来表示地形，类似于栅格的形式，每个点的高度值定义了地形的形状。
trimesh：使用三角形网格来表示地形，可以更精确地定义复杂的地形形状。一般做有地形的训练 比如楼梯、上下坡都设置为trimesh

--------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------
legged_robot.py
def _create_envs(self): 实现创建环境
功能：
1.加载机器人模型，处理刚体模型属性、设置机器人初始状态
2.根据num_envs创建多个环境
3.存储机器人身体不同部位索引

它会调用下述三个函数，下面三个函数用于解析urdf文件中物理参数
  def _process_rigid_shape_props:  用于处理刚体形状的属性
  def _process_dof_props:  用于处理自由度的属性
  def _process_rigid_body_props:  用于处理刚体的属性


--------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------
训练相关函数
核心作用：
step：环境的主要接口，用于接收智能体的动作并推进仿真一步。
1.调用 pre_physics_step 方法，该方法处理动作并将其转换为物理仿真可以使用的格式。
2.更新仿真环境，应用动作并计算新的状态。
3.调用 compute_observations 方法，该方法计算新的观测值。
4.调用 compute_reward 方法，该方法计算奖励值。
5.检查是否需要终止当前仿真步。

重要作用：
_push_robots：给机器人一个随机的基础速度来模拟冲击，用于域随机化训练，增强机器人在不同情况下的鲁棒性。
_update_terrain_curriculum：根据机器人在环境中的行走距离来调整它们所处的地形难度，以此来逐步提高机器人在复杂地形上的表现。
update_command_curriculum：根据机器人追踪速度的表现来调整指令的范围，是另一种课程学习策略，旨在逐步提高机器人对更复杂指令的响应能力。
check_termination：判断当前时间步是否超过了预设的最大时间步 self.max_episode_length。如果超过，则该环境的仿真步数达到上限，需要终止。它综合了接触力、姿态、高度和超时等条件，任何一个条件触发，都会导致该环境的重置。 避免2real时做出危险的行为，重置后会给一个惩罚。


--------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------
算法模块
class LeggedRobotCfgPPO(BaseConfig)
定义新环境时，需要在__init__函数中注册，传入类、配置、PPO类实现与rsl_rl的衔接


--------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------
其他：
1.实现无渲染快速训练：
python legged_gym/scripts/train.py --headless --task go2

2.实现视频录制：
python legged_gym/scripts/play.py --task go2 --load_run 包含config.json和策略的文件夹路径 --record 
play需要手动ctrl+c 或 ctrl+z暂停

3.指定GPU训练：
python legged_gym/scripts/train.py --headless --task go2_field --sim_device cuda:0 --rl_device cuda:0
--sim_device:用于物理模拟的设备
--rl_device:用于强化学习训练的设备

4.decimation参数：
用于控制仿真步长的采样率，多少个时间步进行一次状态更新或奖励计算，小->仿真精度高， 大->计算效率高

5.env_spacing:
初始化机器人时，各机器人之间的间隔距离

6.self_collisions
控制机器人是否启用自碰撞检测

7.class sim:
dt = 0.005 仿真频率，表示0.005秒执行一次仿真
dt × decimation就是一个step的时间
dt 是 时间步长（time step） 的缩写，表示仿真或控制系统中两个连续时间步之间的时间间隔

8.max_episode_length 和 max_episode_length_s
与强化学习训练回合数相关的参数，分别表示每个episode的时间步数和时间长度(s)

9.奖励函数
_prepare_reward_function
计算奖励并返回(num_env,1)这样维度的奖励
